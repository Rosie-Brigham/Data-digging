{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing Project Stats and Cleaning Up Classification Exports\n",
    "\n",
    "When project teams first download classification exports, they usually need to do the following:\n",
    " - Calculate some statistics for classifications, classifiers, and the project in general\n",
    " - Extract just the classifications they want from the raw export (e.g. remove duplicates, classifications of old workflows, and test classifications from before the project was live)\n",
    "\n",
    "This notebook will show you how to do both of these things using Python scripts that exist in this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 2.7.11, numpy version: 1.11.0, pandas version: 0.19.2.\n",
      "Originally developed using Py 2.7.11, np v1.11.0, pd v0.19.2\n",
      "If these versions don't match and stuff breaks, that's probably why.\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ujson\n",
    "\n",
    "print(\"Python version: %d.%d.%d, numpy version: %s, pandas version: %s.\" %(sys.version_info[0], \n",
    "                                                                           sys.version_info[1], \n",
    "                                                                           sys.version_info[2], \n",
    "                                                                           np.__version__, \n",
    "                                                                           pd.__version__))\n",
    "print(\"Originally developed using Py 2.7.11, np v1.11.0, pd v0.19.2\")\n",
    "print(\"If these versions don't match and stuff breaks, that's probably why.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common project statistics are computed as part of `basic_classification_processing`, which used to be `basic_project_stats` (but was renamed because it does more than that)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from basic_classification_processing import basic_stats_processing, basic_stats_help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did for the warm-up notebook \"First Look at Classifications\", we'll assume the project is called \"My Project\".\n",
    "\n",
    "The example file `my-project-classifications.csv` has 50,000 classifications in it, which is enough to extract meaningful statistics but not enough that re-calculating will take very much time.\n",
    "\n",
    "These are real classifications excerpted from a real project, but the `user_id` values have been changed and do not reflect real Zooniverse user IDs. The `user_name` values are real; these are public.\n",
    "\n",
    "## Computing project stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing project stats using:\n",
      "   infile: my-project-classifications.csv\n",
      "Reading classifications from my-project-classifications.csv\n",
      "Considering all classifications in workflow ids:\n",
      "[4958 5030 4975]\n",
      " and workflow_versions:\n",
      "[ 17.6   3.8   1.1]\n",
      "Retaining all non-live classifications in analysis.\n",
      "\n",
      "Overall:\n",
      "\n",
      "50000 classifications of 7568 subjects by 919 classifiers,\n",
      "742 logged in and 177 not logged in, from 949 unique IP addresses.\n",
      "46393 classifications were from logged-in users, 3607 from not-logged-in users.\n",
      "\n",
      "That's 6.61 classifications per subject on average (median = 6.0).\n",
      "The most classified subject has 212 classifications; the least-classified subject has 1.\n",
      "\n",
      "Median number of classifications per user: 18.00\n",
      "Mean number of classifications per user: 54.41\n",
      "\n",
      "Top 10 most prolific classifiers (with classification counts):\n",
      "user_name\n",
      "MerylPG          1028\n",
      "Velski            907\n",
      "Lampyrichard      879\n",
      "DonnaNoble888     770\n",
      "SlowLoris         658\n",
      "Quinacridone      609\n",
      "jrussill          581\n",
      "rcmills1707       579\n",
      "CatzQueenx        550\n",
      "Mgorman           526\n",
      "Name: n_class, dtype: int64\n",
      "\n",
      "\n",
      "Gini coefficient for classifications by user: 0.70\n",
      "\n",
      "Classifications were collected between 2017-09-21 and 2017-09-22.\n",
      "The highest classification id considered here is 71523950.\n",
      "\n",
      "File with ranked list of user classification counts written to my-project-classifications_nclass_byuser_ranked.csv .\n",
      "    and the same, but logged-in only, written to my-project-classifications_nclass_byuser_loggedin_ranked.csv .\n"
     ]
    }
   ],
   "source": [
    "project_name = \"my-project\"\n",
    "\n",
    "classification_file = project_name + \"-classifications.csv\"\n",
    "\n",
    "# if you want to do this separately you need to also import read_classfile from \n",
    "# basic_classification_processing above\n",
    "#classifications = read_classfile(classification_file)\n",
    "\n",
    "# compute the most basic stats, not worrying about duplicate or non-live classifications \n",
    "# or multiple workflows\n",
    "basic_stats_processing(classification_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making sense of the stats output\n",
    "\n",
    "There's a lot of information here. Let's break it down.\n",
    "\n",
    " 1. **Workflow information**\n",
    " -- The first thing the program prints is a list of all workflow IDs with at least one classification in the file, followed by the versions of those workflows (note, it doesn't match them up). This tells you what this export covers. The example \"My Project\" file has 3 workflows, with one version of each, but this can vary by project. \n",
    " \n",
    " 2. **Classification and Classifier counts, averages, medians**\n",
    " -- This is meant to give you a sense of who is classifying and how many classifications they're doing. The IP address count is meant to help you estimate how many of the not-logged-in classifiers are people who went to the project, started classifying, and logged in later (their not-logged-in classifications will have the same IP address and browser session). The difference between means and medians gives you a sense of how skewed the distributions of classifications (per classifier and per subject) are.\n",
    " \n",
    " 3. **Top 10 most prolific classifiers**\n",
    " -- Leaderboards can be useful just to see who's doing what in your project, but *we do not recommend sharing them publicly* because they tend to encourage people to prioritize getting on the leaderboard, sometimes by sacrificing accuracy in favor of classifying fast. Plus, it may not be a great idea to imply that the people who classify the most are also the most valuable volunteers. However, this can be useful internally so you know who your most prolific classifiers are (they aren't always the people who post the most on Talk).\n",
    " \n",
    " 4. **Gini coefficient**\n",
    " -- The Gini coefficient measures inequality in distributions of things. It was originally conceived for economics (e.g. where is the wealth in a country? in the hands of many citizens or a few?), but it's just as applicable to many other fields. In this case we'll use it to see how classifications are distributed among classifiers.\n",
    " \n",
    " G = 0 is a completely even distribution (everyone does the same number of classifications), and ~1 is uneven (~all the classifications are done by one classifier). Typical values of the Gini for healthy Zooniverse projects (Cox et al. 2015) are in the range of 0.7-0.9, although this can vary by project discipline (Spiers et al. 2019). That range is generally indicative of a project with a loyal core group of  volunteers who contribute the bulk of the classification effort, but balanced out by a regular influx of new classifiers trying out the project, from which you continue to draw to maintain a core group of prolific classifiers. Once your project is fairly well established, you can compare it to past Zooniverse projects to see how you're doing.\n",
    " \n",
    " If your G is << 0.7, you may be having trouble recruiting classifiers into a loyal group of volunteers. People are trying it, but not many are staying. If your G is > 0.9, it's a little more complicated. If your total classification count is lower than you'd like it to be, you may be having trouble recruiting classifiers to the project, such that your classification counts are dominated by a few people. But if you have G > 0.9 and plenty of classifications, this may be a sign that your loyal users are -really- committed, so a very high G is not necessarily a bad thing.\n",
    "\n",
    " Of course the Gini coefficient is a simplified measure that doesn't always capture subtle nuances and so forth, but it's still a useful broad metric.\n",
    "\n",
    " 5. **Classification dates and highest classification ID**\n",
    " -- It's useful to make sure your classification dates cover the time period you think they should, and the classification ID may be useful if you need to request a data export for just the most recent classifications since a given classification ID.\n",
    " \n",
    " 6. **File with user classification counts**\n",
    " -- This is always saved when you run the stats file, mainly because it's generated necessarily as a result of computing these stats, and it can come in handy for other things, **e.g. generating an author list**.\n",
    " \n",
    "Note, however, that these are only what the function prints *by default*, and in the default a lot of options are turned off. \n",
    "\n",
    "Let's see the full set of options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Usage: basic_stats_processing(classfile_in, workflow_id=-1, workflow_version=-1, time_elapsed=False, output_csv=False, remove_duplicates=False, keep_nonlive=True, keep_allcols=False, verbose=True)\n",
      "      classifications_infile is a Zooniverse (Panoptes) classifications data export CSV.\n",
      "\n",
      "  Optional inputs:\n",
      "    workflow_id=N\n",
      "       specify the program should only consider classifications from workflow id N\n",
      "    workflow_version=M\n",
      "       specify the program should only consider classifications from workflow version M\n",
      "       (note the program will only consider the major version, i.e. the integer part)\n",
      "    workflow_ver_min=Mmin, workflow_ver_max=Mmax\n",
      "       specify the program should consider classifications from workflow version >= Mmin\n",
      "       or <= Mmax, or Mmin <= workflow version <= Mmax, if both are specified\n",
      "       Note specifying either a min or max supersedes specifying a workflow_version.\n",
      "    outfile_csv=filename.csv\n",
      "       if you want the program to save a sub-file with only classification info from the workflow specified, give the filename here\n",
      "    --time_elapsed\n",
      "       specify the program should compute classification durations and total classification work effort\n",
      "    --remove_duplicates\n",
      "       remove duplicate classifications (subject-user pairs) before analysis.\n",
      "       memory-intensive for big files; probably best to pair with outfile_csv so you save the output.\n",
      "    --keep_nonlive\n",
      "       by default the program ignores classifications made while the project wasn't 'Live'; setting this will keep them in.\n",
      "    --keep_allcols\n",
      "       by default the program only keeps columns required for stats; use this with a specified outfile_csv to save all columns, including annotations. (If you're not using outfile_csv this will just waste memory.)\n",
      "    --silent\n",
      "       by default, the program is verbose and many of the outputs are to screen. Flag this if you just want to print outfiles and not have any screen output.\n",
      "\n",
      "Verbose output will be to stdout (about 1-2 paragraphs' worth).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print help for basic_stats_processing()\n",
    "\n",
    "basic_stats_help()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the different options\n",
    "\n",
    "Let's say we want to run the stats function but only consider classifications from a specific workflow (say, `workflow_id = 5030`) *and* remove duplicate classifications *and* estimate the total amount of human effort the classifiers contributed during those classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing project stats using:\n",
      "   infile: my-project-classifications.csv\n",
      "Reading classifications from my-project-classifications.csv\n",
      "Considering only workflow id 5030\n",
      "Considering all classifications in workflow_versions:\n",
      "[ 3.8]\n",
      "Retaining all non-live classifications in analysis.\n",
      "Found 706 duplicate classifications (3.22 percent of total).\n",
      "Duplicates removed from analysis (465 unique user-subject-workflow groups).\n",
      "\n",
      "Overall:\n",
      "\n",
      "21191 classifications of 2629 subjects by 418 classifiers,\n",
      "338 logged in and 80 not logged in, from 404 unique IP addresses.\n",
      "19915 classifications were from logged-in users, 1276 from not-logged-in users.\n",
      "\n",
      "That's 8.06 classifications per subject on average (median = 7.0).\n",
      "The most classified subject has 141 classifications; the least-classified subject has 1.\n",
      "\n",
      "Median number of classifications per user: 11.00\n",
      "Mean number of classifications per user: 50.70\n",
      "\n",
      "Top 10 most prolific classifiers (with classification counts):\n",
      "user_name\n",
      "Velski          876\n",
      "rcmills1707     579\n",
      "SlowLoris       564\n",
      "Quinacridone    514\n",
      "Kathryn999      492\n",
      "SM-Ystrad       457\n",
      "srasg56         445\n",
      "Mgorman         444\n",
      "grahamg3        412\n",
      "Lampyrichard    340\n",
      "Name: n_class, dtype: int64\n",
      "\n",
      "\n",
      "Gini coefficient for classifications by user: 0.71\n",
      "\n",
      "Classifications were collected between 2017-09-21 and 2017-09-22.\n",
      "The highest classification id considered here is 71504913.\n",
      "\n",
      "Based on 21107 classifications (99.6 percent) where we can probably\n",
      "trust the classification durations, the classifiers spent a total of 3.35 days\n",
      "(or 0.01 years) classifying in the project.\n",
      "\n",
      "Mean classification length:     13.7 seconds\n",
      "Median classification length:    5.1 seconds\n",
      "\n",
      "If we use the mean to extrapolate and include the 0.4 percent of\n",
      "classifications where the reported duration had an error, that means\n",
      "the total time spent is equivalent to 0.01 years of human effort, or\n",
      "0.04 years of FTE (1 person working 40 hours/week, no holiday.)\n",
      "\n",
      "File with ranked list of user classification counts written to my-project-classifications_nclass_byuser_ranked.csv .\n",
      "    and the same, but logged-in only, written to my-project-classifications_nclass_byuser_loggedin_ranked.csv .\n",
      "Saved info for all classifications that have duplicates to my-project-classifications_duplicated_only.csv .\n"
     ]
    }
   ],
   "source": [
    "basic_stats_processing(classification_file, workflow_id=5030, remove_duplicates=True, \n",
    "                       time_elapsed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** how the statistics have changed and are only calculated for the non-duplicated classifications from this specific workflow.\n",
    "\n",
    "**Note 2:** when cleaning duplicates, the program saves the *first* classification from the combination of `user_name + subject_id + workflow_id` that's duplicated.\n",
    "\n",
    "## Cleaning up the classifications export\n",
    "\n",
    "Since we're having to clean up the classifications in order to compute the stats, why not print out that file? Most data reduction codes will require classifications that all have the same structure, i.e., which are all from the same `workflow_id` and `workflow_version`. In addition, it's usually best to ignore duplicate classifications. Rather than doing all this work again when we need to reduce (i.e., aggregate) the data later, we may as well print out the cleaned file now. \n",
    "\n",
    "By default, the program only reads in the columns it needs for the stats, which saves a *lot* of memory when dealing with large files (e.g. millions of classifications). We need to turn off that option in order to produce a file that actually contains the annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading classifications from my-project-classifications.csv\n",
      "File with used subset of classification info written to my-project-classifications-workflow5030-version3.8-nodups.csv .\n"
     ]
    }
   ],
   "source": [
    "# define a new outfile name with some information that will help us later when aggregating the data\n",
    "outfile_cleanclass = project_name + \"-classifications-workflow5030-version3.8-nodups.csv\"\n",
    "\n",
    "# since we've already printed the stats to the screen above, don't re-print them\n",
    "basic_stats_processing(classification_file, workflow_id=5030, workflow_version=3.8, \n",
    "                       remove_duplicates=True, outfile_csv=outfile_cleanclass, \n",
    "                       keep_allcols=True, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on using the `basic_classification_processing` scripts\n",
    "\n",
    "`basic_classification_processing.py` is written so that you can do a few things independently of computing *all* the stats, but also you can use the `basic_stats_processing()` function to do everything you need. You can also run it from the command line: try \n",
    "\n",
    " `%> python basic_classification_processing.py` \n",
    "\n",
    "to see the syntax in that case. You can run this script each time you are about to aggregate your data on the latest classification export to produce a clean export with no duplicates (and only live classifications, if you wish) or to split a full project export into multiple based on workflows, or to extract only a specific workflow version from a workflow-specific extract. And, with each of these, if you keep `verbose` on, you'll get the stats for free."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
